{
 "metadata": {
  "name": "",
  "signature": "sha256:fdd273d73ded52f15cfe7e32c54e3869ae5114c76d30ec6b39f5b27bf2384253"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Active Appearance Model (AAMs)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Active Appearance Models (AAMs) are generative parametric models that describe the shape and appearance of a certain object class; e.g., the human face. In a typical application, these models are matched against input images to obtain the set of parameters that best describe a particular instance of the object being modelled. \n",
      "\n",
      "The aim of this notebook is to showcase the basic functionality provided by the package `menpo.aam`."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Build a simple AAM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We start by loading the set of landmarked images that we will be used to build the AAM; in this case, the LFPW training set. Note that the images are first cropped (in order to save valuable memory space), then rescaled to a consistent face size (necessary for correctly extracting features) and, finally, converted to grayscale (note that the grayscale convertion is optional, RGB images can also be used and, as a matter of fact, in principle, any `n` channel image representation (such as depth and shape images or feature images such as HoG) can also be used)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import menpo.io as pio\n",
      "from menpo.landmark import labeller, ibug_68_points, ibug_68_trimesh\n",
      "\n",
      "images = []\n",
      "# load landmarked imarges\n",
      "for i in pio.import_images('/Users/joan/PhD/DataBases/lfpw/trainset/*.png'):\n",
      "    # crop them\n",
      "    i.crop_to_landmarks_proportion(0.1)\n",
      "    # convert them to greyscale\n",
      "    if i.n_channels == 3:\n",
      "        images.append(i.as_greyscale(mode='luminosity'))\n",
      "    else:\n",
      "        images.append(i)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "#visualize the first image\n",
      "images[0].landmarks['PTS'].view()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we will use the `aambuilder` function to build an AAM using the previous images. Most of the time, in order to do that we will need to define a dictionary whose fields specify the type of AAM that will be built. Although, in this first example, there is no real need to explicitely define the entire dictionary, since we will be using a fairly standard setting, however, for the sake of clarity we will define such dictionary explicitely stating all possible options."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from menpo.aam.base import aam_builder\n",
      "from menpo.transform.piecewiseaffine import PiecewiseAffine\n",
      "\n",
      "# set options. Most options are set to their default \n",
      "# values. Obviously, it is in not required to set all \n",
      "# the options to their default values but this is \n",
      "# done here in favour of clarity\n",
      "build_options = {'group': 'PTS', \n",
      "                 'label': 'all',\n",
      "                 'interpolator': 'scipy',\n",
      "                 'diagonal_range': None,\n",
      "                 'boundary': 3,\n",
      "                 'transform_cls': PiecewiseAffine,\n",
      "                 'trilist': None,\n",
      "                 'patch_size': None,\n",
      "                 'n_levels': 1, \n",
      "                 'downscale': 2,\n",
      "                 'scaled_reference_frames': False,\n",
      "                 'feature_type': None,\n",
      "                 'max_shape_components': 25,\n",
      "                 'max_appearance_components': 250}\n",
      "# build aam\n",
      "aam = aam_builder(images, **build_options)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The returned AAM object allows us to, for example, generate random AAM instances. Note that we can also specify particular shape and appearance parameters values from which to generate the instance. \n",
      "\n",
      "Notes: \n",
      "\n",
      "- In the final verison of this Notebook one should be able to also print the AAM object."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "# random instance\n",
      "aam.instance().view() \n",
      "# random instance\n",
      "aam.random_instance().view_new() \n",
      "# handpicked intance\n",
      "aam.instance(shape_weights=[2,1.5,-1], \n",
      "             appearance_weights=[2,2,-3,-3,3]).view_new()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As with any other python object we can also save the AAM for later use using `pickle`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "\n",
      "# set path\n",
      "path =  '/data/PhD/Models/'\n",
      "# set name\n",
      "name = 'aam_' + 'lfpw_' + 'default'\n",
      "\n",
      "# save aam\n",
      "pickle.dump({'aam': aam}, open(path + name, 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And load it back."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "\n",
      "# set path\n",
      "path =  '/data/PhD/Models/'\n",
      "# set name\n",
      "name = 'aam_' + 'lfpw_' + 'default'\n",
      "\n",
      "# load aam\n",
      "obj = pickle.load(open(path + name, \"rb\"))\n",
      "aam = obj['aam']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Fit a simple AAM "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "More importantly, we can now fit the AAM object to a novel image using all the methods in the `menpo.lucaskanade` package. We will start by loading the novel image (i.e. an image that was not used for building the AAM), for example, the BreakingBad image in the Menpo's data folder. Note that the current implementation requires the image to be greyscale (because the AAM was built using greayscale images) in order to be fitted. Finally, we also crop the BreakindBad image to make the landmarks easily visible to the user. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "breakingbad = pio.import_builtin_asset('breakingbad.jpg')\n",
      "breakingbad = breakingbad.as_greyscale(mode='luminosity')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "breakingbad.landmarks['PTS'].view()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "breakingbad.crop_to_landmarks_proportion(0.3, group='PTS')\n",
      "breakingbad.landmarks['PTS'].view()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we will need to initialize the AAM Lucas-Kanade fitting object. We are allowed to set different options regarding the fitting procedure, from the specific algorithm to be used to the number of components that will be optimized. Although in this example we will only use the default fitting setting, for the sake of clarity, we will define an explicit dictionary containing all the options."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from menpo.aam.fitter import LucasKanadeAAMFitter\n",
      "from menpo.lucaskanade.appearance import AlternatingInverseCompositional\n",
      "from menpo.lucaskanade.residual import LSIntensity\n",
      "from menpo.transform.modeldriven import OrthoMDTransform\n",
      "from menpo.transform.affine import Similarity\n",
      "\n",
      "# set fitting options. As before we favour \n",
      "# clarity by explicitely stating all possible\n",
      "# options\n",
      "fitting_setting_options = \\\n",
      "    {'lk_algorithm': AlternatingInverseCompositional,\n",
      "     'residual': LSIntensity,\n",
      "     'md_transform_cls': OrthoMDTransform,\n",
      "     'global_transform_cls': Similarity,\n",
      "     'n_shape': 6, \n",
      "     'n_appearance': 100}\n",
      "\n",
      "# initialize Lucas-Kanade aam fitting\n",
      "lk_aam_fitter = LucasKanadeAAMFitter(aam, **fitting_setting_options)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now attempt to fit the greayscale version of the BreakingBad image using the method `fit_image` in the LK AAM fitter object. Note that, because the BreakingBad image is annotated, we will initialize the fitting procedure by perturbing the original ground truth annotations with white noise. Again, in order to favor clarity, an explicit dictionary will be used to set this method's options (do not worry, there will not be more explicit dictionaries)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fitting_options = {'group': 'PTS',\n",
      "                   'label': 'all',\n",
      "                   'noise_std': 0.0,\n",
      "                   'max_iters': 60,\n",
      "                   'rotation': False,\n",
      "                   'verbose': True, \n",
      "                   'view': True}\n",
      "\n",
      "# fit test image\n",
      "aam_fitting = lk_aam_fitter.fit_image(breakingbad, **fitting_options)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not surprisingly the alignment did not quite work. Do not worry! This was just the most basic AAM we could build ;-).\n",
      "\n",
      "Note that the `fit_image` method returns a Fitting object. Apart from containing the final result, i.e. the fitted shape, Fitting objects allow us to analyse and visualize interesting aspects of the fitting procedure, e.g:\n",
      "\n",
      "- Obtain the final fitting result:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "final_shape = aam_fitting.final_shape\n",
      "\n",
      "%matplotlib inline\n",
      "final_shape.view()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Visualize it on top of the fitted image:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "aam_fitting.view_final_fitting()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Visualize the initial position from which the fitting started:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "aam_fitting.view_initialization()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Visualize the sequence of warped images:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib qt\n",
      "aam_fitting.view_warped_images()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Visualize the sequence of apperance reconstructions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib qt\n",
      "aam_fitting.view_appearance_reconstructions()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Visualize the sequence of error images:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib qt\n",
      "aam_fitting.view_error_images()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Plot the error evolution, i.e. the error between the ground truth annotation and the fitted shape per iteration. Note that this is possible only because the original image is annotated."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "aam_fitting.plot_error(color_list=['b'], marker_list=['*'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Building a more powerful AAM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `ammbuilder` function allows us to automatically build many different types of AAMs by simply specifying the right options. \n",
      "\n",
      "For example, using the same LFPW training data and similar fitting options as before, we can build and fit a Multiresolution HoG-AAM that will have no problem in correctly fitting the BreakingBad image:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# delete previous aam in order to save \n",
      "# valuable memory space\n",
      "del aam"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# label images\n",
      "labeller(images, 'PTS', ibug_68_trimesh);\n",
      "\n",
      "def hog_closure(image):\n",
      "    return image.features.hog(window_step_vertical=3, window_step_horizontal=3)\n",
      "\n",
      "# build aam\n",
      "aam = aam_builder(images, \n",
      "                  group='PTS', \n",
      "                  trilist=images[0].landmarks['ibug_68_trimesh'].lms.trilist, \n",
      "                  n_levels=3, \n",
      "                  downscale=1.5, \n",
      "                  feature_type=hog_closure, \n",
      "                  max_shape_components=25, \n",
      "                  max_appearance_components=250)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# lowest resolution level\n",
      "%matplotlib inline\n",
      "aam.instance(level=0).view(channels=0) \n",
      "# middle resolution level\n",
      "aam.instance(level=1).view_new(channels=0)\n",
      "# highest resolution level\n",
      "aam.instance().view_new(channels=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initialize Lucas-Kanade aam fitting\n",
      "lk_aam_fitter = LucasKanadeAAMFitter(aam, \n",
      "                                     n_shape=[3, 6, 12], \n",
      "                                     n_appearance=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# fit test images\n",
      "aam_fitting = lk_aam_fitter.fit_image(breakingbad, \n",
      "                                      max_iters=60)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "aam_fitting.view_initialization()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "aam_fitting.view_final_fitting()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib qt\n",
      "aam_fitting.view_warped_images()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib qt\n",
      "aam_fitting.view_error_images(channels=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "aam_fitting.plot_error(color_list=['b'], marker_list=['*'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
