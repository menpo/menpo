{
 "metadata": {
  "name": "ICCV 2013 3D Lucas-Kanade"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.io\n",
      "import numpy as np\n",
      "from menpo.warp import warp\n",
      "from menpo.warp.base import map_coordinates_interpolator, matlab_interpolator\n",
      "from menpo.transform import Affine\n",
      "from menpo.align.lucaskanade import InverseCompositional\n",
      "from menpo.align.lucaskanade.similaritymeasure import *\n",
      "import matplotlib.pyplot as plt\n",
      "from menpo.convolution import log_gabor\n",
      "from collections import OrderedDict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_rms_point_error(test_pts, template_affine, M):\n",
      "    iteration_pts = M.apply(template_affine.T)\n",
      "    diff_pts = test_pts - iteration_pts\n",
      "    return np.sqrt(np.mean(diff_pts ** 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def my_test_affine(tdata, pt_offsets, alg_list, n_iters, n_freq_tests, spatial_sigma, interpolator, verbose):\n",
      "    # tdata has three fields:\n",
      "    #     tdata.img1      greyscale target image\n",
      "    #     tdata.img2      greyscale template image\n",
      "    #     tdata.tmplt     [x_start y_start x_end y_end] template rectangle corners\n",
      "    #\n",
      "    # pt_offsets is a N x 6 matrix of (x1, y1, x2, y2, x3, y3) deltas for each\n",
      "    # of the three affine control points. Zero mean, unit variance,\n",
      "    #     pt_offsets = np.random.rand(N, 6)\n",
      "    #\n",
      "    # alg_list is an ordered dictionary of similarity measures to use:\n",
      "    #   {'String measure name', (MeasureClass, constructor params, constructor keyword arguments)}\n",
      "    # eg.\n",
      "    #   {'LeastSquares', (LeastSquares, None, None)}\n",
      "\n",
      "    # Target affine warp control points - triangle on the rectangle\n",
      "    \n",
      "    minX = tdata.tmplt[0]\n",
      "    minY = tdata.tmplt[1]\n",
      "    minZ = tdata.tmplt[2]\n",
      "    maxX = tdata.tmplt[3]\n",
      "    maxY = tdata.tmplt[4]\n",
      "    maxZ = tdata.tmplt[5]\n",
      "\n",
      "\n",
      "    target_affine = np.array([[minX, minY, minZ],\n",
      "                              [maxX, minY, minZ],\n",
      "                              [minX + ((maxX - minX) / 2.0) - 0.5, maxY, minZ],\n",
      "                              [minX + ((maxX - minX) / 2.0) - 0.5, minY + ((maxY - minY) / 2.0) - 0.5, maxZ]])\n",
      "\n",
      "    # Template image dimensions (formerly had +1)\n",
      "    template_width = maxX - minX\n",
      "    template_height = maxY - minY\n",
      "    template_depth = maxZ - minZ\n",
      "    tmplt_shape = (template_height, template_width, template_depth)\n",
      "\n",
      "    # Template affine warp control points\n",
      "    template_affine = np.array([[0,                  0,                   0],      \n",
      "                                [template_width,     0,                   0],\n",
      "                                [template_width / 2.0, template_height,   0],\n",
      "                                [template_width / 2.0, template_height / 2.0, template_depth]]).T\n",
      "\n",
      "    # Initial warp parameters. Unperturbed translation\n",
      "    p_init = np.eye(4)\n",
      "    p_init[:, 3] = [tdata.tmplt[0] - 1, tdata.tmplt[1] - 1,\n",
      "                    tdata.tmplt[2] - 1, 1]\n",
      "\n",
      "    # Translate by 0.5 pixels to avoid identity warp. Warping introduces a little\n",
      "    # smoothing and this avoids the case where the first iteration for a forwards\n",
      "    # algorithm is on the \"unsmoothed\" unwarped image\n",
      "    p_init[0, 3] += 0.5\n",
      "    p_init[1, 3] += 0.5\n",
      "    p_init[2, 3] += 0.5\n",
      "\n",
      "    # Pick a total of n_freq_tests point offsets from pt_offsets randomly\n",
      "    ind = np.floor(pt_offsets.shape[0] * np.random.rand(n_freq_tests, 1)).astype(np.uint32)\n",
      "    ind[ind == 0] = 1\n",
      "    # Uncomment this line to randomise the noise added\n",
      "    # pt_offsets1 = pt_offsets[ind, :]\n",
      "    pt_offsets1 = pt_offsets\n",
      "\n",
      "    # Scale point offsets to have required sigma\n",
      "    pt_offsets1 = pt_offsets1 * spatial_sigma\n",
      "\n",
      "    img = tdata.img1\n",
      "    results = {}\n",
      "\n",
      "    if verbose:\n",
      "        plt.close()\n",
      "        f = plt.figure()\n",
      "        plt.ion()\n",
      "        f.add_subplot(2, 1, 0)\n",
      "        img_plot = plt.imshow(img[:, :, 0], cmap=plt.cm.Greys_r)\n",
      "        f.add_subplot(2, 1, 1)\n",
      "\n",
      "    # Run\n",
      "    for offset_idx in xrange(n_freq_tests):\n",
      "        # Test points: apply current point offset to target points\n",
      "        test_pts = target_affine + np.reshape(pt_offsets1[offset_idx, :], [4,3])\n",
      "        # Solve for affine warp\n",
      "        M1 = np.vstack([template_affine, np.ones([1, template_affine.shape[1]])]).T\n",
      "        M2 = np.vstack([test_pts.T, np.ones([1, test_pts.shape[0]])]).T\n",
      "        M = np.linalg.solve(M1, M2).T\n",
      "\n",
      "        # Warp original image to get test \"template\" image\n",
      "        tmplt = warp(tdata.img2, tmplt_shape, Affine(M), interpolator=interpolator)\n",
      "        if verbose:\n",
      "            tmplt_plot = plt.imshow(tmplt[:, :, 0], cmap=plt.cm.Greys_r)\n",
      "\n",
      "        if verbose:\n",
      "            print 'Initial RMS: {0}'.format(compute_rms_point_error(test_pts, template_affine, Affine(p_init)))\n",
      "\n",
      "        # Run each algorithm\n",
      "        for sim_measure in alg_list:\n",
      "            # Allow the passing of arguments in to the instantiated class\n",
      "            clz = alg_list[sim_measure][0]\n",
      "            args = alg_list[sim_measure][1]\n",
      "            kwargs = alg_list[sim_measure][2]\n",
      "\n",
      "            if not args is None and not kwargs is None:\n",
      "                metric = clz(*args, **kwargs)\n",
      "            elif not args is None:\n",
      "                metric = clz(*args)\n",
      "            elif not kwargs is None:\n",
      "                metric = clz(**kwargs)\n",
      "            else:\n",
      "                metric = clz()\n",
      "\n",
      "            final_transform = InverseCompositional(img, tmplt, metric, Affine(p_init), interpolator=interpolator).align(max_iters=n_iters)\n",
      "            rms_pt_error = compute_rms_point_error(test_pts, template_affine, final_transform)\n",
      "\n",
      "            if verbose:\n",
      "                print '{0}: {1}'.format(sim_measure, rms_pt_error)\n",
      "\n",
      "                img_plot.set_array(img[:, :, 0])\n",
      "                tmplt_plot.set_array(tmplt[:, :, 0])\n",
      "                plt.draw()\n",
      "\n",
      "            if not sim_measure in results:\n",
      "                results[sim_measure] = []\n",
      "            measure_results = results[sim_measure]\n",
      "            measure_results.append(rms_pt_error)\n",
      "            results[sim_measure] = measure_results\n",
      "\n",
      "    return results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load datasets\n",
      "np.set_printoptions(suppress=False, linewidth=600)\n",
      "pt_offset = scipy.io.loadmat('/home/pts08/Dropbox/lk/face_recognition_2011/data/affine_pt_offset_3d.mat')['pt_offset']\n",
      "visual_human = scipy.io.loadmat('/home/pts08/Dropbox/lk/face_recognition_2011/data/visual_human_small.mat')\n",
      "\n",
      "coords = visual_human['tmplt1'][0]\n",
      "tmplt = np.array(visual_human['tmplt_img'], dtype=np.float64)\n",
      "\n",
      "# Pre-compute\n",
      "tmplt_width = coords[4] - coords[1]\n",
      "tmplt_height = coords[3] - coords[0]\n",
      "tmplt_depth = coords[5] - coords[2]\n",
      "\n",
      "# Set up experiment variables\n",
      "verbose = True\n",
      "n_iters = 30                     # Number of gradient descent iterations\n",
      "n_freq_tests = 30                # Number of frequency of convergence tests\n",
      "max_spatial_error = 3.0          # Max location error for deciding convergence\n",
      "all_spc_sig = np.arange(1, 11)   # All spatial sigmas (1,10)\n",
      "interpolator = map_coordinates_interpolator\n",
      "# Gabor variables\n",
      "blank_image = np.ones([tmplt_width, tmplt_height, tmplt_depth])\n",
      "\n",
      "alg_list = OrderedDict([('LeastSquares', (LeastSquares, None, None)),\n",
      "                        ('ECC', (ECC, None, None)),\n",
      "                        #'IRLS': (IRLS, None, None),\n",
      "                        ('GaborFourier', (GaborFourier, None, None)),\n",
      "                        ('GradientImages', (GradientImages, None, None)),\n",
      "                        ('GradientCorrelation', (GradientCorrelation, None, None))])\n",
      "\n",
      "results = np.zeros([len(all_spc_sig), len(alg_list)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run experiment\n",
      "tdata = lambda x: 0\n",
      "tdata.img1 = tmplt\n",
      "tdata.img2 = tmplt\n",
      "tdata.tmplt = coords\n",
      "\n",
      "# Matrix S for Gabor-Fourier method\n",
      "# Thanks to Peter Kovesi's Gabor Filters\n",
      "# http://www.csse.uwa.edu.au/~pk/\n",
      "S = log_gabor(blank_image, num_scales=32)[2]\n",
      "alg_list['GaborFourier'] = (GaborFourier, [blank_image.shape], {'filter_bank': S})\n",
      "\n",
      "# Run tests\n",
      "for sigma_ind, current_sigma in enumerate(all_spc_sig):\n",
      "    res = my_test_affine(tdata, pt_offset, alg_list, n_iters, n_freq_tests, current_sigma, interpolator, verbose)\n",
      "\n",
      "    for measure_ind, sim_measure in enumerate(alg_list):\n",
      "        measure_results = res[sim_measure]\n",
      "        # Get whether or not it converges\n",
      "        n_converge = len(filter(lambda error: error < max_spatial_error, measure_results))\n",
      "        results[sigma_ind, measure_ind] = n_converge\n",
      "\n",
      "\n",
      "# Save out results just in case\n",
      "scipy.io.savemat('3dresults.mat', {'results': results})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot results\n",
      "mean_results = np.mean(np.mean(results, 1), 0) / float(n_freq_tests)\n",
      "\n",
      "\n",
      "line_styles = ['k--D', 'y:^', 'r:*', 'g:^', 'b-s']\n",
      "lines = []\n",
      "for i in xrange(mean_results.shape[1]):\n",
      "    lines.append(all_spc_sig)\n",
      "    lines.append(mean_results[:, i])\n",
      "    lines.append(line_styles[i])\n",
      "\n",
      "p = plt.plot(*lines)\n",
      "\n",
      "legend_labels = [a for a in alg_list.keys()]\n",
      "plt.yticks(np.linspace(0, 1, 11))\n",
      "plt.xticks(all_spc_sig)\n",
      "plt.ylabel('Frequency of Convergence')\n",
      "plt.xlabel('Point Standard Deviation')\n",
      "plt.legend(p, legend_labels)\n",
      "plt.title('Yale: with Smoothing')\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
