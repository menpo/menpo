{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# AAMs for Image Alignment - shape and appearance\n",
      "\n",
      "### 1. Load the data\n",
      "\n",
      "Use the **autoimporter** to automatically load the *annotated* test images of the *Labelled Faces Parts in the Wild (LFPW)* dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pybug.io import auto_import\n",
      "from pybug.landmark.labels import  (labeller, ibug_68_points, \n",
      "                                    ibug_68_contour, ibug_68_trimesh)\n",
      "from pybug.shape import PointCloud\n",
      "\n",
      "# load the training images of the LFPW database as landmarked images using the autoimporter\n",
      "images = auto_import('/vol/atlas/databases/lfpw/testset/' + '*.png', max_images=10)\n",
      "\n",
      "# label the landmarks using the ibug's \"standard\" 68 points mark-up\n",
      "labeller(images, 'PTS', ibug_68_points)\n",
      "labeller(images, 'PTS', ibug_68_contour)\n",
      "labeller(images, 'PTS', ibug_68_trimesh)\n",
      "\n",
      "# Extract shape data. -1 because the annotation are 1 based\n",
      "points = [img.landmarks['PTS'].lms.points - 1  for img in images]\n",
      "shapes = [PointCloud(p) for p in points]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use **pickle** to load a previously build *AAM*:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "\n",
      "# load a previously buid AAM\n",
      "aam = pickle.load(open('/vol/atlas/aams/lfpw_pwa', \"rb\"))\n",
      "\n",
      "reference_frame = aam[\"reference_frame\"]\n",
      "appearance_model = aam[\"appearance_model\"]\n",
      "shape_model = aam[\"shape_model\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2. Fit\n",
      "\n",
      "Randomly select an image to fit:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "# select a test image at random\n",
      "ind = np.random.randint(len(images))\n",
      "test_image = images[ind].as_greyscale()\n",
      "test_shape = shapes[ind]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pybug.image import MaskedNDImage\n",
      "test_image2 = MaskedNDImage(test_image.pixels, mask=test_image.mask)\n",
      "test_image2.landmarks = test_image.landmarks\n",
      "test_image = test_image2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_image.landmarks.view()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_shape.view()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualize ground truth landmarks and warped appearance for the selected test image:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pybug.transform.piecewiseaffine import PiecewiseAffineTransform\n",
      "from pybug.shape import TriMesh\n",
      "\n",
      "source = reference_frame.landmarks['ibug_68_trimesh'].lms\n",
      "\n",
      "template_pwa = PiecewiseAffineTransform(source, test_shape)\n",
      "template = test_image.warp_to(reference_frame.mask, template_pwa)\n",
      "\n",
      "test_image.landmarks[\"ground_truth\"] = test_shape\n",
      "test_image.landmarks['ground_truth'].view_new()\n",
      "gcf().set_size_inches((10,10))\n",
      "\n",
      "template.landmarks['reference'] = source\n",
      "template.landmarks['reference'].view_new()\n",
      "gcf().set_size_inches((6,6))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Initialize the fitting procedure by perturbing the true scale and translation parameters of the manually annotated shape:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pybug.transform.affine import SimilarityTransform\n",
      "import numpy as np\n",
      "\n",
      "# create a transform in the model space to perturb the fitting\n",
      "model_transform = SimilarityTransform.from_vector(0.1 * np.random.randn(4)) \n",
      "#print 'pertubation of the mean:'\n",
      "#print model_transform\n",
      "\n",
      "# perturb the mean by the transform, and find the optimal (incorrect) transform from the \n",
      "# model to the ground truth test_shape\n",
      "peturbed_mean = model_transform.apply(shape_model.mean)\n",
      "# this transform is obviously non-optimal (because of the pertubation, and the error is\n",
      "# not dependent on the scale of the target, as we performed the pertubation in model space.\n",
      "initial_global_transform  = SimilarityTransform.align(peturbed_mean, test_shape)\n",
      "#print '\\ninital transform we will set:'\n",
      "#print initial_global_transform\n",
      "\n",
      "optimal_global_transform  = SimilarityTransform.align(shape_model.mean, test_shape)\n",
      "#print '\\nthe optimal transform is:'\n",
      "#print optimal_global_transform"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Build a Model Driven Transform using the AAM's shape model and Piece Wise Affine (PWA) warps: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pybug.transform.modeldriven import GlobalMDTransform\n",
      "from pybug.transform.affine import SimilarityTransform\n",
      "\n",
      "model_driven_transform = GlobalMDTransform(shape_model, PiecewiseAffineTransform, initial_global_transform,\n",
      "                                           source=source)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_image.landmarks['initial'] = model_driven_transform.target\n",
      "test_image.landmarks[\"initial\"].view_new()\n",
      "gcf().set_size_inches((10,10))\n",
      "\n",
      "initial_warped_image = test_image.warp_to(template.mask, model_driven_transform)\n",
      "initial_warped_image.landmarks['reference'] =  source\n",
      "initial_warped_image.landmarks['reference'].view_new()\n",
      "gcf().set_size_inches((6,6))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Build three \"fitter\" objects using the three variations of the Project Out algorithm:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pybug.lucaskanade.residual import (LSIntensity, ECC, GaborFourier, \n",
      "                                              GradientImages, GradientCorrelation)\n",
      "from pybug.lucaskanade.appearance import (AlternatingForwardAdditive, AlternatingForwardCompositional,\n",
      "                                           AlternatingInverseCompositional)\n",
      "from copy import deepcopy\n",
      "\n",
      "# standard Least Squares Residual\n",
      "residual = LSIntensity()\n",
      "\n",
      "# forward additive\n",
      "fa = AlternatingForwardAdditive(appearance_model, residual, deepcopy(model_driven_transform))\n",
      "# forward compositional\n",
      "# TODO there is a bug here\n",
      "fc = AlternatingForwardCompositional(appearance_model, residual, deepcopy(model_driven_transform))\n",
      "# inverse compositional\n",
      "ic = AlternatingInverseCompositional(appearance_model, residual, deepcopy(model_driven_transform))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fit the image using all three different methods:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%prun optimal_transform_1 = fa.align(test_image, model_driven_transform.as_vector(), max_iters=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%prun optimal_transform_2 = fc.align(test_image, model_driven_transform.as_vector(), max_iters=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%prun optimal_transform_3 = ic.align(test_image, model_driven_transform.as_vector(), max_iters=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualize the results:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fitted_appearance_1 = test_image.warp_to(template.mask, optimal_transform_1)\n",
      "fitted_appearance_2 = test_image.warp_to(template.mask, optimal_transform_2)\n",
      "fitted_appearance_3 = test_image.warp_to(template.mask, optimal_transform_3)\n",
      "\n",
      "fitted_appearance_1.landmarks['reference'] = source\n",
      "fitted_appearance_1.landmarks['reference'].view_new()\n",
      "gcf().set_size_inches((6,6))\n",
      "\n",
      "fitted_appearance_2.landmarks['reference'] = source\n",
      "fitted_appearance_2.landmarks['reference'].view_new()\n",
      "gcf().set_size_inches((6,6))\n",
      "\n",
      "fitted_appearance_3.landmarks['reference'] = source\n",
      "fitted_appearance_3.landmarks['reference'].view_new()\n",
      "gcf().set_size_inches((6,6))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_image.landmarks['fa'] = optimal_transform_1.target\n",
      "test_image.landmarks['fc'] = optimal_transform_2.target\n",
      "test_image.landmarks['ic'] = optimal_transform_3.target\n",
      "\n",
      "test_image.landmarks['fa'].view_new()\n",
      "gcf().set_size_inches((10,10))\n",
      "\n",
      "test_image.landmarks['fc'].view_new()\n",
      "gcf().set_size_inches((10,10))\n",
      "\n",
      "test_image.landmarks['ic'].view_new()\n",
      "gcf().set_size_inches((10,10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}